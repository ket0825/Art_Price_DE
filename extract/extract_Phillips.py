import requests
import json
from bs4 import BeautifulSoup
import re
import time
import random
import concurrent.futures  # ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“ˆ

# Phillips API ê¸°ë³¸ URL
MAKER_ID = 10800  # 6740
BASE_URL = f"https://api.phillips.com/api/maker/{MAKER_ID}/lots"

# User-Agent ë¦¬ìŠ¤íŠ¸ (ëœë¤ ì„ íƒ)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:94.0) Gecko/20100101 Firefox/94.0",
]

HEADERS = {
    "User-Agent": random.choice(USER_AGENTS),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Referer": "https://www.phillips.com/browse",
    "Connection": "keep-alive",
}

# ì„¸ì…˜ ìœ ì§€ (ì¿ í‚¤ í™•ë³´)
session = requests.Session()
session.headers.update(HEADERS)

# Phillips í™ˆí˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ ì¿ í‚¤ ì €ì¥ (403 ë°©ì§€)
session.get("https://www.phillips.com/")
cookies = session.cookies.get_dict()  # ì¿ í‚¤ í™•ì¸

def fetch_detail_info(detail_url):
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    if detail_url == "No URL":
        return {}

    # ìš”ì²­ ì „ì— ì¼ì •í•œ ë”œë ˆì´ ì¶”ê°€ (403 ë°©ì§€)
    #time.sleep(random.uniform(2, 5))

    # User-Agent ë³€ê²½
    session.headers.update({"User-Agent": random.choice(USER_AGENTS)})

    # ì¿ í‚¤ í¬í•¨í•˜ì—¬ ìš”ì²­
    response = session.get(detail_url, cookies=cookies)

    if response.status_code == 403:
        print(f"âš ï¸ [403 Forbidden] Access denied to {detail_url}")
        return {}

    if response.status_code != 200:
        print(f"âš ï¸ Failed to fetch details from {detail_url}. Status: {response.status_code}")
        return {}

    soup = BeautifulSoup(response.text, "html.parser")

    detail_info = {
        "year": None,
        "artwork_type": None,
        "height_cm": None,
        "width_cm": None,
        "edition": None,
    }

    # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
    additional_info_elem = soup.select_one(".lot-page__lot__additional-info")

    if additional_info_elem:
        additional_info_text = additional_info_elem.get_text(separator=" ").strip()

        # ì œì‘ ì—°ë„ ì¶”ì¶œ (4ìë¦¬ ìˆ«ì íƒìƒ‰)
        year_match = re.search(r"(\b\d{4}\b)", additional_info_text)
        if year_match:
            detail_info["year"] = int(year_match.group(1))

        # ë§¤ì²´ ì¶”ì¶œ
        material_match = re.search(r"(oil|watercolour|lithograph|screenprint|graphite|ink|acrylic|mixed media|tempera|gouache|charcoal|pastel|crayon|pencil|\
                                   plate|ceramic|earthenware|Linocut|Aquatint|drypoint|Etching|Engraving)", additional_info_text, re.IGNORECASE)
        if material_match:
            detail_info["artwork_type"] = material_match.group(0).strip()

        # í¬ê¸° ì •ë³´ ì¶”ì¶œ (23.5 x 15.2 cm)
        size_match = re.search(r"(\d+(\.\d+)?)\s*x\s*(\d+(\.\d+)?)\s*cm", additional_info_text)
        if size_match:
            detail_info["height_cm"] = float(size_match.group(1))
            detail_info["width_cm"] = float(size_match.group(3))

        # ì—ë””ì…˜ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: Edition of 100)
        edition_match = re.search(r"edition of (\d+)", additional_info_text, re.IGNORECASE)
        if edition_match:
            detail_info["edition"] = int(edition_match.group(1))

    return detail_info

def fetch_lots():
    """ê²½ë§¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    auction_site = "Phillips"
    auction_data = []
    detail_urls = []

    # í˜ì´ì§€ë„¤ì´ì…˜ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
    page = 1
    total_pages = None  

    while True:
        # API ìš”ì²­ ì‹œ í•„ìš”í•œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°
        params = {
            "page": page, #í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸
            "resultsperpage": 24, #í•œ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ ë°ì´í„° ê°œìˆ˜
            "lotStatus": "past" #ê³¼ê±° ê²½ë§¤ ë°ì´í„°
        }

        # API ìš”ì²­ ë³´ë‚´ê¸°
        response = session.get(BASE_URL, headers=HEADERS, params=params)

        if response.status_code == 200:
            data = response.json() # ì‘ë‹µ ë°ì´í„°ë¥¼ JSONí˜•ì‹ìœ¼ë¡œ ë³€í™˜

            # ì „ì²´ í˜ì´ì§€ ìˆ˜ ì„¤ì • (ì²« ìš”ì²­ì—ì„œë§Œ ê°€ì ¸ì˜´), totalPagesê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1ë¡œ ì„¤ì •
            if total_pages is None:
                total_pages = data.get("totalPages", 1)
                print(f"total page is {total_pages}")

            print(f"ğŸ“Œ Fetching page {page} of {total_pages}...")

            for item in data.get("data", []):
                detail_url = item.get("detailLink", "No URL")

                # ë¹„ì •ìƒì  ê²½ë§¤ ì¢…ë£Œ ì‹œê°„ ì²˜ë¦¬
                auction_start = item.get("auctionStartDateTimeOffset", "0001-01-01T00:00:00")
                auction_end = item.get("auctionEndDateTimeOffset", "0001-01-01T00:00:00")
                if "0001" in auction_end:
                    if "0001" in auction_start:
                        continue
                    auction_end = auction_start

                # ë¹„ì •ìƒì  ê²½ë§¤ê°€ê²© ì²˜ë¦¬
                price = item.get("hammerPlusBP", 0)
                if price is None or price == 0:
                    continue

                # ê¸°ë³¸ ì •ë³´ ì €ì¥
                lot_info = {
                    "artist": item.get("makerName", "Unknown Artist"),
                    "title": item.get("description", "No Title"),
                    "start_date": auction_start,
                    "end_date": auction_end,
                    "low_estimate": item.get("lowEstimate", 0),
                    "high_estimate": item.get("highEstimate", 0),
                    "price": price,
                    "auction_site": auction_site,
                    "year": None,
                    "artwork_type": None,
                    "edition": None,
                    "height_cm": None,
                    "width_cm": None,
                    "currency": item.get("currencySign", ""),
                }

                detail_urls.append((lot_info, detail_url))

            page += 1
            if page > total_pages:
                break
        else:
            print(f"âš ï¸ Failed to fetch data on page {page}. Status code: {response.status_code}")
            break

    # ë³‘ë ¬ë¡œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        future_to_lot = {executor.submit(fetch_detail_info, detail_url): lot_info for lot_info, detail_url in detail_urls}

        extracted = 1
        results = []
        for future in concurrent.futures.as_completed(future_to_lot):
            lot_info = future_to_lot[future]
            try:
                detail_data = future.result()
                lot_info.update(detail_data)
                results.append(lot_info)
                print(extracted)
                extracted += 1
            except Exception as e:
                print(f"âš ï¸ Error fetching details: {e}")

    # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ ë³‘í•©
    for i, detail_data in enumerate(results):
        detail_urls[i][0].update(detail_data)
        auction_data.append(detail_urls[i][0])

    return auction_data

# ë°ì´í„° í¬ë¡¤ë§ ì‹¤í–‰
auction_results = fetch_lots()

# JSON íŒŒì¼ë¡œ ì €ì¥
json_filename = f"./data/phillips_auction_results_{MAKER_ID}.json"
with open(json_filename, "w", encoding="utf-8") as file:
    json.dump(auction_results, file, indent=4, ensure_ascii=False)

print(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {json_filename}")
